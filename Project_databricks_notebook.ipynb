{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f113466-44c9-422e-8088-6172a9dcca13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: 'File uploaded to /FileStore/tables/Damages_use.csv\\nFile uploaded to /FileStore/tables/Endorse_use.csv\\nFile uploaded to /FileStore/tables/Charges_use.csv\\nFile uploaded to /FileStore/tables/Restrict_use.csv\\nFile uploaded to /FileStore/tables/Primary_Person_use.csv\\nFile uploaded to /FileStore/tables/Units_use.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"File uploaded to /FileStore/tables/Damages_use.csv\n",
    "File uploaded to /FileStore/tables/Endorse_use.csv\n",
    "File uploaded to /FileStore/tables/Charges_use.csv\n",
    "File uploaded to /FileStore/tables/Restrict_use.csv\n",
    "File uploaded to /FileStore/tables/Primary_Person_use.csv\n",
    "File uploaded to /FileStore/tables/Units_use.csv\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb31efd8-af23-42d5-8ea1-4660b25dcce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "damages_df = spark.read.csv(\"/FileStore/tables/Damages_use.csv\", header=True, inferSchema=True)\n",
    "endorse_df = spark.read.csv(\"/FileStore/tables/Endorse_use.csv\", header=True, inferSchema=True)\n",
    "restrict_df = spark.read.csv(\"/FileStore/tables/Restrict_use.csv\", header=True, inferSchema=True)\n",
    "primary_person_df = spark.read.csv(\"/FileStore/tables/Primary_Person_use.csv\", header=True, inferSchema=True)\n",
    "units_df = spark.read.csv(\"/FileStore/tables/Units_use.csv\", header=True, inferSchema=True)\n",
    "charges_df = spark.read.csv(\"/FileStore/tables/Charges_use.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731dd359-ce12-428f-99a1-a1e30a90ebed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22eb9823-f373-45d1-923e-15cc5e8539d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|CRASH_ID|death_count|\n+--------+-----------+\n|15379024|          2|\n|15429998|          2|\n+--------+-----------+\n\n+--------+---------------+\n|crash_id|tot_death_count|\n+--------+---------------+\n|15129823|              3|\n|14952803|              4|\n|15070158|              3|\n+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# This DF reports the number of males killed in accidents. This DF only takes the death of primary persons in consideration, as data for non-primary persons involved in the crash is not available.\n",
    "primary_person_df.select(primary_person_df['CRASH_ID'],primary_person_df['UNIT_NBR'],primary_person_df['PRSN_NBR'],primary_person_df['PRSN_GNDR_ID'],primary_person_df['PRSN_INJRY_SEV_ID']).filter((primary_person_df['PRSN_GNDR_ID']=='MALE') & (primary_person_df[\"PRSN_INJRY_SEV_ID\"]==\"KILLED\")).groupBy(primary_person_df[\"CRASH_ID\"]).agg(count(\"*\").alias(\"death_count\")).sort(col(\"death_count\").desc()).filter(col(\"death_count\")>=2).show()\n",
    "\n",
    "#This DF is an alternate solution, assuming missing data. Unable to filter on gender as units_table does not capture gender details\n",
    "units_df.select('crash_id','unit_nbr','death_cnt').distinct().groupBy('crash_id').agg(sum('death_cnt').alias(\"tot_death_count\")).filter(col(\"tot_death_count\")>2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d245da83-4ec3-4371-a846-0a4c273c1c94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "primary_person = primary_person_df.createOrReplaceTempView(\"primary_person\")\n",
    "units = units_df.createOrReplaceTempView(\"units\")\n",
    "damages= damages_df.createOrReplaceTempView(\"damages\")\n",
    "charges = charges_df.createOrReplaceTempView(\"charges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37c230a-09b0-4c66-a6f2-928a133d17bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|crash_id|count(1)|\n+--------+--------+\n+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select crash_id, count(*) from primary_person where prsn_gndr_id='MALE' and PRSN_INJRY_SEV_ID\n",
    "='KILLED' group by crash_id having count(*)>2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a6172f-1f2f-47c5-abdc-4dc8d8e8f35a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        crashes_with_male_deaths_df = primary_person_df.filter(\n",
    "            (col(\"prsn_gndr_id\") == \"MALE\") &\n",
    "            (col(\"PRSN_INJRY_SEV_ID\") == \"KILLED\")\n",
    "        ).groupBy(\"crash_id\").agg(count(\"*\").alias(\"male_death_count\"))\n",
    "\n",
    "        # Filter for crashes with more than 2 male fatalities\n",
    "        result_df = crashes_with_male_deaths_df.filter(col(\"male_death_count\") > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17be59e5-3157-4452-9b1c-35baa051698f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 396"
     ]
    }
   ],
   "source": [
    "# This DF reports the number of motorcycles booked for crashes. Assumptions - Police bikes cannot be booked, Booked motorcycles have an entry in the charges dataset\n",
    "units_df.join(charges_df,(units_df['CRASH_ID']==charges_df['CRASH_ID']) & (units_df['UNIT_NBR']==charges_df['UNIT_NBR']),\"inner\").filter((units_df[\"VEH_BODY_STYL_ID\"]=='MOTORCYCLE') | (units_df[\"VEH_BODY_STYL_ID\"]=='POLICE MOTORCYCLE')).select(units_df[\"CRASH_ID\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447faa88-8a18-45a7-84a5-19f80ea74d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: <bound method DataFrame.filter of DataFrame[CRASH_ID: int, UNIT_NBR: int, UNIT_DESC_ID: string, VEH_PARKED_FL: string, VEH_HNR_FL: string, VEH_LIC_STATE_ID: string, VIN: string, VEH_MOD_YEAR: string, VEH_COLOR_ID: string, VEH_MAKE_ID: string, VEH_MOD_ID: string, VEH_BODY_STYL_ID: string, EMER_RESPNDR_FL: string, OWNR_ZIP: string, FIN_RESP_PROOF_ID: string, FIN_RESP_TYPE_ID: string, VEH_DMAG_AREA_1_ID: string, VEH_DMAG_SCL_1_ID: string, FORCE_DIR_1_ID: string, VEH_DMAG_AREA_2_ID: string, VEH_DMAG_SCL_2_ID: string, FORCE_DIR_2_ID: string, VEH_INVENTORIED_FL: string, VEH_TRANSP_NAME: string, VEH_TRANSP_DEST: string, CONTRIB_FACTR_1_ID: string, CONTRIB_FACTR_2_ID: string, CONTRIB_FACTR_P1_ID: string, VEH_TRVL_DIR_ID: string, FIRST_HARM_EVT_INV_ID: string, INCAP_INJRY_CNT: int, NONINCAP_INJRY_CNT: int, POSS_INJRY_CNT: int, NON_INJRY_CNT: int, UNKN_INJRY_CNT: int, TOT_INJRY_CNT: int, DEATH_CNT: int]>"
     ]
    }
   ],
   "source": [
    "units_df.filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91b10bd-d142-4c5a-b8e5-2420032beacf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 773"
     ]
    }
   ],
   "source": [
    "# This DF outputs the number of motorcycles booked for accidents. Future check - check if charge_id and unit_id are composite keys for this dataset\n",
    "\n",
    "booked_units = charges_df.select(\"crash_id\",\"unit_nbr\").distinct()\n",
    "units_df.join(\n",
    "    booked_units,\n",
    "    (units_df['CRASH_ID'] == booked_units['CRASH_ID']) & (units_df['UNIT_NBR'] == booked_units['UNIT_NBR']),\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (units_df[\"VEH_BODY_STYL_ID\"] == 'MOTORCYCLE') | (units_df[\"VEH_BODY_STYL_ID\"] == 'POLICE MOTORCYCLE')\n",
    ").select(units_df[\"CRASH_ID\"],units_df['UNIT_NBR']).distinct().count()\n",
    "\n",
    "\n",
    "# This DF is more straightforward\n",
    "units_df.filter(col(\"VEH_BODY_STYL_ID\").contains(\"MOTORCYCLE\")).select(\"crash_id\",\"unit_nbr\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1818b62d-a03b-4a63-9e6e-bce5cb52c6ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 116110"
     ]
    }
   ],
   "source": [
    "charges_df.select(\"crash_id\",\"unit_nbr\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b680259-319b-489c-8441-a2954d7618f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|  VEH_MAKE_ID|Count|\n+-------------+-----+\n|        BUICK|    1|\n|         HINO|    1|\n|        ACURA|    1|\n|INTERNATIONAL|    1|\n|       TOYOTA|    1|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"with crash as (select crash_id, unit_nbr from primary_person where prsn_type_id=\"DRIVER\" and PRSN_INJRY_SEV_ID='KILLED' and  PRSN_AIRBAG_ID = 'NOT DEPLOYED'), units as (select distinct crash_id, unit_nbr,VEH_MAKE_ID from units) select VEH_MAKE_ID, count(*) as Count from (select crash.crash_id, crash.unit_nbr,units.VEH_MAKE_ID from crash inner join units on crash.crash_id = units.crash_id) group by VEH_MAKE_ID order by Count limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e2db0e-c338-4641-9971-a96b988da3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n|veh_make_id|cnt|\n+-----------+---+\n|  CHEVROLET| 16|\n|       FORD| 12|\n|         NA|  7|\n|      DODGE|  7|\n|     NISSAN|  5|\n+-----------+---+\n\n+-----------+---+\n|veh_make_id|cnt|\n+-----------+---+\n|  CHEVROLET| 16|\n|       FORD| 12|\n|         NA|  7|\n|      DODGE|  7|\n|     NISSAN|  5|\n+-----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# This SQL code outputs the top 5 vehicle makes of the cars present in the crashes in which the driver died and airbags did not deploy\n",
    "spark.sql(\"\"\"select veh_make_id, count(*) as cnt from (select distinct crash_id, unit_nbr,veh_make_id from units where units.crash_id in (select crash_id from primary_person where prsn_type_id=\"DRIVER\" and PRSN_INJRY_SEV_ID='KILLED' and  PRSN_AIRBAG_ID = 'NOT DEPLOYED')) group by veh_make_id order by cnt desc limit 5\"\"\").show()\n",
    "\n",
    "# This DF achieves the same result \n",
    "\n",
    "filtered_primary_person_df = primary_person_df.filter(\n",
    "    (col(\"prsn_type_id\") == \"DRIVER\") &\n",
    "    (col(\"PRSN_INJRY_SEV_ID\") == \"KILLED\") &\n",
    "    (col(\"PRSN_AIRBAG_ID\") == \"NOT DEPLOYED\")\n",
    ").select(\"crash_id\")\n",
    "\n",
    "joined_df = units_df.join(filtered_primary_person_df,(units_df['crash_id'] == filtered_primary_person_df['crash_id']),'inner').select(units_df[\"crash_id\"], \"unit_nbr\", \"veh_make_id\").distinct()\n",
    "\n",
    "result_df = joined_df.groupBy(\"veh_make_id\").agg(count(\"*\").alias(\"cnt\")).orderBy(\"cnt\", ascending=False).limit(5)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13b0637-5fdc-43c1-aa59-1037229a507d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|  147956|\n+--------+\n\n+------+\n| count|\n+------+\n|147956|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "#This Spark SQL code outputs the number of vehicles involved in hit and runs where the driver had a valid drivers license\n",
    "spark.sql(\"\"\"select count(*) from primary_person inner join (select distinct crash_id,veh_hnr_fl from units) units_hnr on primary_person.crash_id = units_hnr.crash_id where prsn_type_id='DRIVER' and DRVR_LIC_CLS_ID != 'UNLICENSED'\"\"\" ).show()\n",
    "\n",
    "#This DF achieves the same result \n",
    "\n",
    "units_distinct_df = spark.read.table(\"units\").select(\"crash_id\", \"veh_hnr_fl\").distinct()\n",
    "\n",
    "# Join tables\n",
    "joined_df = primary_person_df.join(\n",
    "    units_distinct_df,\n",
    "    primary_person_df['crash_id'] == units_distinct_df['crash_id'],\n",
    "    'inner'\n",
    ").filter(\n",
    "    (col(\"prsn_type_id\") == \"DRIVER\") &\n",
    "    (col(\"DRVR_LIC_CLS_ID\") != \"UNLICENSED\")\n",
    ")\n",
    "\n",
    "# Count the occurrences\n",
    "result_df = joined_df.agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33ce7b0-32ec-43a5-aa73-cd9c57e4e203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e3b60e-da14-409c-a2a5-ed6206ec1132",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n|DRVR_LIC_STATE_ID|state_count|\n+-----------------+-----------+\n|            Texas|      34842|\n+-----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# This Spark SQL code outputs the state with the highest accidents in which women were not involved\n",
    "#spark.sql(\"\"\"select drvr_lic_state_id, count(*) as state_count from (select distinct crash_id, drvr_lic_state_id from primary_person where crash_id not in (select distinct crash_id from primary_person where prsn_gndr_id='FEMALE') and drvr_lic_cls_id !='OTHER/OUT OF STATE\n",
    "#') group by drvr_lic_state_id order by state_count desc limit 1\"\"\").show()\n",
    "\n",
    "#This DF achieves the same result\n",
    "spark.sql(\"\"\"select distinct crash_id, drvr_lic_state_id from primary_person where crash_id not in (select distinct crash_id from primary_person where prsn_gndr_id='FEMALE') and drvr_lic_cls_id !='OTHER/OUT OF STATE'\"\"\").count()\n",
    "\n",
    "female_drivers = primary_person_df.filter(col(\"prsn_gndr_id\")=='FEMALE').select('crash_id').distinct() \n",
    "\n",
    "primary_person_df.join(female_drivers,primary_person_df['crash_id']==female_drivers['crash_id'],\"leftanti\").select(primary_person_df[\"crash_id\"],'DRVR_LIC_STATE_ID').distinct().groupBy(col('DRVR_LIC_STATE_ID')).agg(count('DRVR_LIC_STATE_ID').alias('state_count')).orderBy(col(\"state_count\").desc()).limit(1).show()\n",
    "\n",
    "# Main query to get drvr_lic_state_id counts\n",
    "#result_df = non_female_crash_ids.filter(col('DRVR_LIC_CLS_ID')!='OTHER/OUT OF STATE').groupBy(col('DRVR_LIC_STATE_ID')).agg(count('DRVR_LIC_STATE_ID').alias('state_count')).orderBy(col(\"state_count\").desc()).limit(1)\n",
    "# Show the result\n",
    "#result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203155a3-55b8-471a-98b9-8b937dd66916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b14268-51e0-4eab-af6a-ce3194beea68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|veh_make_id|sum_cas|\n+-----------+-------+\n|     TOYOTA|   4149|\n|      DODGE|   3098|\n|     NISSAN|   3079|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# This Spark SQL code outputs the 3-5th vehicle makes that cause the most accidents\n",
    "#spark.sql(\"\"\"select veh_make_id, sum(total_cas) as sum_cas from (select crash_id,veh_make_id,TOT_INJRY_CNT+DEATH_CNT as total_cas from (select distinct crash_id,TOT_INJRY_CNT,DEATH_CNT,veh_make_id from units where veh_make_id!='NA')) group by veh_make_id order by sum_cas desc limit 5 offset 2\"\"\").show()\n",
    "\n",
    "#This DF outputs the same result\n",
    "\n",
    "distinct_cas=units_df.filter(col(\"veh_make_id\") != \"NA\").select(\"crash_id\",\"veh_make_id\",\"TOT_INJRY_CNT\",\"DEATH_CNT\").distinct()\n",
    "\n",
    "total_cas_df = distinct_cas.select('crash_id','veh_make_id',(col('tot_injry_cnt')+col('death_cnt')).alias(\"tot_cas_cnt\")).groupBy(\"veh_make_id\").agg(sum('tot_cas_cnt').alias(\"sum_cas\")).orderBy(col('sum_cas').desc()).limit(5)\n",
    "\n",
    "\n",
    "total_cas_df.limit(5).subtract(total_cas_df.limit(2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb12dbc-3dd6-4140-aa87-a8921aa7461e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n|PRSN_ETHNICITY_ID|    VEH_BODY_STYL_ID|\n+-----------------+--------------------+\n|            WHITE|           AMBULANCE|\n|            BLACK|                 BUS|\n|            WHITE|      FARM EQUIPMENT|\n|            WHITE|          FIRE TRUCK|\n|            WHITE|          MOTORCYCLE|\n|            WHITE|NEV-NEIGHBORHOOD ...|\n|            WHITE|PASSENGER CAR, 2-...|\n|            WHITE|PASSENGER CAR, 4-...|\n|            WHITE|              PICKUP|\n|            WHITE|    POLICE CAR/TRUCK|\n|            WHITE|   POLICE MOTORCYCLE|\n|            WHITE|SPORT UTILITY VEH...|\n|            WHITE|               TRUCK|\n|            WHITE|       TRUCK TRACTOR|\n|            WHITE|             UNKNOWN|\n|            WHITE|                 VAN|\n|            BLACK|   YELLOW SCHOOL BUS|\n+-----------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#top ethnicity for each vehicle body style\n",
    "#spark.sql(\"\"\"select prsn_ethnicity_id, veh_body_styl_id from (select PRSN_ETHNICITY_ID, veh_body_styl_id, row_number() over (partition by veh_body_styl_id order by cnt desc) as row_num from (select prsn_ethnicity_id, veh_body_styl_id, count(*) as cnt from (select p.crash_id, p.PRSN_ETHNICITY_ID\n",
    "#, u.unit_nbr, u.veh_body_styl_id from primary_person p inner join \n",
    "#(select distinct crash_id, unit_nbr,VEH_BODY_STYL_ID from units where VEH_BODY_STYL_ID\n",
    "#not in ('NA','OTHER  (EXPLAIN IN NARRATIVE)','NOT REPORTED')) u\n",
    "##on p.crash_id = u.crash_id and p.unit_nbr = u.unit_nbr\n",
    "#where p.prsn_ethnicity_id not in (\"NA\", \"UNKNOWN\")) group by PRSN_ETHNICITY_ID, veh_body_styl_id)) where row_num =1\"\"\").display()\n",
    "\n",
    "# dataframe api code\n",
    "filtered_units_df = units_df.filter(\n",
    "    col(\"VEH_BODY_STYL_ID\").isin(\"NA\", \"OTHER  (EXPLAIN IN NARRATIVE)\", \"NOT REPORTED\") == False\n",
    ").select(\"crash_id\", \"unit_nbr\", \"VEH_BODY_STYL_ID\").distinct()\n",
    "\n",
    "# Join tables\n",
    "joined_df = primary_person_df.join(\n",
    "    filtered_units_df,\n",
    "    (primary_person_df['crash_id'] == filtered_units_df['crash_id']) & (primary_person_df['unit_nbr'] == filtered_units_df['unit_nbr'])\n",
    ").filter(\n",
    "    col(\"prsn_ethnicity_id\").isin(\"NA\", \"UNKNOWN\") == False\n",
    ").select(\"PRSN_ETHNICITY_ID\", \"VEH_BODY_STYL_ID\")\n",
    "\n",
    "# Group by PRSN_ETHNICITY_ID and VEH_BODY_STYL_ID and count occurrences\n",
    "grouped_df = joined_df.groupBy(\"PRSN_ETHNICITY_ID\", \"VEH_BODY_STYL_ID\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "# Use window function to assign row numbers based on the count\n",
    "windowSpec = Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"cnt\").desc())\n",
    "ranked_df = grouped_df.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter rows with row number 1\n",
    "result_df = ranked_df.filter(col(\"row_num\") == 1).select(\"PRSN_ETHNICITY_ID\", \"VEH_BODY_STYL_ID\")\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904547cc-17b0-417d-a52f-690dbedd10bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n|drvr_zip|tot_count|\n+--------+---------+\n|   76010|       65|\n|   78521|       58|\n|   75067|       58|\n|   78542|       46|\n|   77084|       46|\n+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#analysis 8\n",
    "#spark.sql(\"\"\"select drvr_zip, count(*) as tot_count from (select crash_id,drvr_zip from (select crash_id, drvr_zip, row_number() over (partition by crash_id, drvr_zip order by unit_nbr) as row_num from primary_person) where row_num = 1 and crash_id in (select distinct crash_id from units where CONTRIB_FACTR_1_ID like '%ALCOHOL%' or CONTRIB_FACTR_2_ID like '%ALCOHOL%' or CONTRIB_FACTR_P1_ID like '%ALCOHOL%') and drvr_zip is not null ) group by drvr_zip order by tot_count desc limit 5\"\"\").show()\n",
    "alcohol_crash_ids_df = units_df.filter(\n",
    "    (col(\"CONTRIB_FACTR_1_ID\").like(\"%ALCOHOL%\")) |\n",
    "    (col(\"CONTRIB_FACTR_2_ID\").like(\"%ALCOHOL%\")) |\n",
    "    (col(\"CONTRIB_FACTR_P1_ID\").like(\"%ALCOHOL%\"))\n",
    ").select(\"crash_id\").distinct()\n",
    "#dataframe code \n",
    "joined_df = primary_person_df.join(\n",
    "    alcohol_crash_ids_df,\n",
    "    primary_person_df['crash_id'] == alcohol_crash_ids_df['crash_id']\n",
    ").select(primary_person_df[\"crash_id\"], \"drvr_zip\", \"unit_nbr\")\n",
    "\n",
    "# Use window function to assign row numbers based on crash_id and drvr_zip\n",
    "windowSpec = Window.partitionBy(\"crash_id\", \"drvr_zip\").orderBy(\"unit_nbr\")\n",
    "ranked_df = joined_df.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter rows with row number 1 and non-null drvr_zip\n",
    "filtered_df = ranked_df.filter((col(\"row_num\") == 1) & (col(\"drvr_zip\").isNotNull()))\n",
    "\n",
    "# Group by drvr_zip and count occurrences\n",
    "result_df = filtered_df.groupBy(\"drvr_zip\").agg(count(\"*\").alias(\"tot_count\")).orderBy(col(\"tot_count\").desc()).limit(5)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f22d207-75d7-4ef0-8ff7-f5e1cf7c9ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-72306166561033>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mselect crash_id,  (select distinct crash_id from units where CONTRIB_FACTR_1_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m or CONTRIB_FACTR_2_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m or CONTRIB_FACTR_P1_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `crash_id` cannot be resolved. ; line 1 pos 7;\n",
       "'Project ['crash_id, scalar-subquery#1960 [] AS scalarsubquery()#1961]\n",
       ":  +- Distinct\n",
       ":     +- Project [crash_id#168]\n",
       ":        +- Filter ((CONTRIB_FACTR_1_ID#193 LIKE %ALCOHOL% OR CONTRIB_FACTR_2_ID#194 LIKE %ALCOHOL%) OR CONTRIB_FACTR_P1_ID#195 LIKE %ALCOHOL%)\n",
       ":           +- SubqueryAlias units\n",
       ":              +- View (`units`, [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,VEH_TRANSP_DEST#192,CONTRIB_FACTR_1_ID#193,CONTRIB_FACTR_2_ID#194,CONTRIB_FACTR_P1_ID#195,VEH_TRVL_DIR_ID#196,FIRST_HARM_EVT_INV_ID#197,INCAP_INJRY_CNT#198,NONINCAP_INJRY_CNT#199,POSS_INJRY_CNT#200,NON_INJRY_CNT#201,UNKN_INJRY_CNT#202,TOT_INJRY_CNT#203,DEATH_CNT#204])\n",
       ":                 +- Relation [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,... 13 more fields] csv\n",
       "+- OneRowRelation\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-72306166561033>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mselect crash_id,  (select distinct crash_id from units where CONTRIB_FACTR_1_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m or CONTRIB_FACTR_2_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m or CONTRIB_FACTR_P1_ID like \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mALCOHOL\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `crash_id` cannot be resolved. ; line 1 pos 7;\n'Project ['crash_id, scalar-subquery#1960 [] AS scalarsubquery()#1961]\n:  +- Distinct\n:     +- Project [crash_id#168]\n:        +- Filter ((CONTRIB_FACTR_1_ID#193 LIKE %ALCOHOL% OR CONTRIB_FACTR_2_ID#194 LIKE %ALCOHOL%) OR CONTRIB_FACTR_P1_ID#195 LIKE %ALCOHOL%)\n:           +- SubqueryAlias units\n:              +- View (`units`, [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,VEH_TRANSP_DEST#192,CONTRIB_FACTR_1_ID#193,CONTRIB_FACTR_2_ID#194,CONTRIB_FACTR_P1_ID#195,VEH_TRVL_DIR_ID#196,FIRST_HARM_EVT_INV_ID#197,INCAP_INJRY_CNT#198,NONINCAP_INJRY_CNT#199,POSS_INJRY_CNT#200,NON_INJRY_CNT#201,UNKN_INJRY_CNT#202,TOT_INJRY_CNT#203,DEATH_CNT#204])\n:                 +- Relation [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,... 13 more fields] csv\n+- OneRowRelation\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `crash_id` cannot be resolved. ; line 1 pos 7;\n'Project ['crash_id, scalar-subquery#1960 [] AS scalarsubquery()#1961]\n:  +- Distinct\n:     +- Project [crash_id#168]\n:        +- Filter ((CONTRIB_FACTR_1_ID#193 LIKE %ALCOHOL% OR CONTRIB_FACTR_2_ID#194 LIKE %ALCOHOL%) OR CONTRIB_FACTR_P1_ID#195 LIKE %ALCOHOL%)\n:           +- SubqueryAlias units\n:              +- View (`units`, [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,VEH_TRANSP_DEST#192,CONTRIB_FACTR_1_ID#193,CONTRIB_FACTR_2_ID#194,CONTRIB_FACTR_P1_ID#195,VEH_TRVL_DIR_ID#196,FIRST_HARM_EVT_INV_ID#197,INCAP_INJRY_CNT#198,NONINCAP_INJRY_CNT#199,POSS_INJRY_CNT#200,NON_INJRY_CNT#201,UNKN_INJRY_CNT#202,TOT_INJRY_CNT#203,DEATH_CNT#204])\n:                 +- Relation [CRASH_ID#168,UNIT_NBR#169,UNIT_DESC_ID#170,VEH_PARKED_FL#171,VEH_HNR_FL#172,VEH_LIC_STATE_ID#173,VIN#174,VEH_MOD_YEAR#175,VEH_COLOR_ID#176,VEH_MAKE_ID#177,VEH_MOD_ID#178,VEH_BODY_STYL_ID#179,EMER_RESPNDR_FL#180,OWNR_ZIP#181,FIN_RESP_PROOF_ID#182,FIN_RESP_TYPE_ID#183,VEH_DMAG_AREA_1_ID#184,VEH_DMAG_SCL_1_ID#185,FORCE_DIR_1_ID#186,VEH_DMAG_AREA_2_ID#187,VEH_DMAG_SCL_2_ID#188,FORCE_DIR_2_ID#189,VEH_INVENTORIED_FL#190,VEH_TRANSP_NAME#191,... 13 more fields] csv\n+- OneRowRelation\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"select crash_id,  (select distinct crash_id from units where CONTRIB_FACTR_1_ID like '%ALCOHOL%' or CONTRIB_FACTR_2_ID like '%ALCOHOL%' or CONTRIB_FACTR_P1_ID like '%ALCOHOL%')\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961f912e-8443-48e1-82c7-a43e8b268994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8849\n"
     ]
    }
   ],
   "source": [
    "#analysis 9 \n",
    "spark.sql(\"\"\"select distinct crash_id from units where crash_id not in (select crash_id from damages) and (VEH_DMAG_SCL_1_ID in ('DAMAGED 5','DAMAGED 6','DAMAGED 7 HIGHEST') or VEH_DMAG_SCL_2_ID\n",
    "in ('DAMAGED 5','DAMAGED 6','DAMAGED 7 HIGHEST')) and FIN_RESP_TYPE_ID like '%INSURANCE%'\"\"\").count()\n",
    "\n",
    "\n",
    "#dataframe code\n",
    "# Filter out distinct crash_id where crash_id is not in damages and VEH_DMAG_SCL_1_ID or VEH_DMAG_SCL_2_ID contains specified values\n",
    "filtered_units_df = units_df.filter(\n",
    "    (\n",
    "        (col(\"VEH_DMAG_SCL_1_ID\").isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\")) |\n",
    "        (col(\"VEH_DMAG_SCL_2_ID\").isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"))\n",
    "    ) &\n",
    "    (col(\"FIN_RESP_TYPE_ID\").like(\"%INSURANCE%\"))\n",
    ")\n",
    "\n",
    "no_damage_df = filtered_units_df.join(damages_df, filtered_units_df['crash_id']==damages_df['crash_id'],'leftanti')\n",
    "\n",
    "# Count the distinct crash_ids\n",
    "count = no_damage_df.select(\"crash_id\").distinct().count()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca84e15-c2f5-4ee0-8613-3a4ff169fe95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|VEH_MAKE_ID|make_count|\n+-----------+----------+\n|       FORD|     18021|\n|  CHEVROLET|     15917|\n|     TOYOTA|     11286|\n|      DODGE|      7423|\n|     NISSAN|      6945|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#analysis 10 \n",
    "spark.sql(\"\"\"with top_state as (select veh_lic_state_id, count(*) as state_count from (select distinct crash_id,veh_lic_state_id from units) where veh_lic_state_id not in ('Unknown','NA') group by veh_lic_state_id order by state_count desc limit 25), top_colour as (select veh_color_id, count(*) as color_count from (select distinct crash_id,unit_nbr, veh_color_id from units where veh_color_id != 'NA') group by veh_color_id order by color_count desc limit 10)\n",
    "select units.VEH_MAKE_ID, count(*) as make_count from primary_person inner join (select distinct crash_id,VEH_MAKE_ID,unit_nbr, veh_color_id, veh_lic_state_id from units) units on primary_person.crash_id = units.crash_id \n",
    "inner join \n",
    "(select * from charges where charge like '%SPEED%') charge\n",
    "on primary_person.crash_id = charge.crash_id\n",
    "where primary_person.DRVR_LIC_CLS_ID like '%CLASS%' and units.veh_color_id in (select veh_color_id from top_colour) and units.veh_lic_state_id in (select veh_lic_state_id from top_state)\n",
    "group by units.VEH_MAKE_ID\n",
    "order by make_count desc\n",
    "limit 5\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec83826-b058-43f7-b0af-262daf4858b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|VEH_MAKE_ID|make_count|\n+-----------+----------+\n|       FORD|     18021|\n|  CHEVROLET|     15917|\n|     TOYOTA|     11286|\n|      DODGE|      7423|\n|     NISSAN|      6945|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#df code for analysis 10 \n",
    "top_state_df = units_df.filter(~col(\"veh_lic_state_id\").isin(\"NA\")) \\\n",
    "    .select(\"crash_id\", \"veh_lic_state_id\").distinct() \\\n",
    "    .groupBy(\"veh_lic_state_id\").agg(count(\"*\").alias(\"state_count\")) \\\n",
    "    .orderBy(col(\"state_count\").desc()).limit(25)\n",
    "\n",
    "top_colour_df = units_df.filter(col(\"veh_color_id\") != \"NA\") \\\n",
    "    .select(\"crash_id\", \"unit_nbr\", \"veh_color_id\").distinct() \\\n",
    "    .groupBy(\"veh_color_id\").agg(count(\"*\").alias(\"color_count\")) \\\n",
    "    .orderBy(col(\"color_count\").desc()).limit(10)\n",
    "\n",
    "# Main Query\n",
    "make_count_df = primary_person_df.join(\n",
    "    units_df.select(\"crash_id\", \"VEH_MAKE_ID\", \"unit_nbr\", \"veh_color_id\", \"veh_lic_state_id\").distinct(),\n",
    "    \"crash_id\"\n",
    ").join(\n",
    "    charges_df.filter(col(\"charge\").like(\"%SPEED%\")).select(\"crash_id\"),\n",
    "    \"crash_id\"\n",
    ").join(\n",
    "    top_colour_df.select(\"veh_color_id\"),\n",
    "    \"veh_color_id\"\n",
    ").join(\n",
    "    top_state_df.select(\"veh_lic_state_id\"),\n",
    "    \"veh_lic_state_id\"\n",
    ").filter(\n",
    "    col(\"DRVR_LIC_CLS_ID\").like(\"%CLASS%\")\n",
    ").groupBy(\"VEH_MAKE_ID\").agg(count(\"*\").alias(\"make_count\")) \\\n",
    ".orderBy(col(\"make_count\").desc()).limit(5)\n",
    "\n",
    "make_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291aeaaf-f78d-4217-93b0-f283b5203256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-2118443724326295>:1\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    spark.sql(\"\"\"select)\u001B[0m\n",
       "\u001B[0m                        ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOF while scanning triple-quoted string literal\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-2118443724326295>:1\u001B[0;36m\u001B[0m\n\u001B[0;31m    spark.sql(\"\"\"select)\u001B[0m\n\u001B[0m                        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOF while scanning triple-quoted string literal\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: EOF while scanning triple-quoted string literal (<command-2118443724326295>, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"select)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BCG Case Study",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
